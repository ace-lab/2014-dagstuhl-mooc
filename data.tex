\section{New data policies should be collectively negotiated}

MOOCs generate vast amounts of data: data about the content of the
course (videos, quizzes, exercises, slides, \ldots{}), data about the
students
(clickstream data, answers to questions, discussion forums, \ldots) and data
about the professor and his or her pedagogical team. This section is
concerned with the last two points, i.e. user data.  As any user data
collected by other online platforms, MOOCs raise serious concerns on
possible breaches of the user's privacy. These are addressed in the
first point. Nonetheless, these datasets do also constitute great
opportunities. At the individual level, this data is necessary for
adapting learning activities to individual needs. At the collective
level, the data supports extracting knowledge about the effectiveness of
the MOOC components, in order to improve the concerned MOOC or to
acquire general pedagogical knowledge. These opportunities are
emphasized in the second point.
      	

\subsection{How can we trust MOOC platforms?}

Data privacy is a complex issue that cannot be fully developed here, but
we would however like to point out the current opacity of data in MOOC
platforms.  To know what is being recorded, users may carefully read the
``terms of services'' document between the MOOC platforms and the
University. These are complex documents, hardly read by users anyhow,
and some of them do not clearly state what is being recorded. We
recommend to apply a general principle: learners' data belong to the
learner. Accordingly to this principle, the learners' data (mainly
his/her interaction traces) has to be manageable by the learner: easy
access and visualisation of any data that has been recorded about him,
the ability to share his/her data with others, ability to make requests
on his/her data to get indicators, ability to analyze his/her
data\ldots{} Especially, the learner should be allowed to delete any
subset
of their data: he will in this case loose the above mentioned advantages
of personal data, but that would be a personal choice of users.

The rationale for this individual ownership of personal data is that
stored data can sometimes be detrimental to learners. A trivial example
is when a potential employer would access performance data for a
candidate who followed MOOCs. Another concern is that a record of
failure may reduce the expectations that a teacher has about a learner:
if the teacher has lower expectation, it is well known, under the label
`Rosenthal effect', that students adapt to these lower expectations by
lower achievement.

This proposal can be compared with the medical field, where Patient's
Digital Medical Record contains highly sensitive data, and where any
break in confidentiality may have some consequences to the patient,
medical consequences as well as social or economical consequences, such
as stigmatisation or impact of insurance contract. Both, students and
patients take ``exams''. In most countries, patient's data are owned by
patients themselves, with instruments providing guarantees in terms of
security, reliability, and confidentiality. Patients may accept to share
their data to a doctor or a nurse for their personal health purpose. It
may also happen that a patient after informed and prior consent may
accept (or may not) to share their data for scientific purposes,
e.g. participation in a clinical study. But informed consent does not
mean only to click on a box ``I Accept'' after a long list of almost not
readable conditions. And if the patient refuses to participate in the
clinical study, he keeps his right to be cured with highest
standards. It must be similar for students enrolled in MOOCs. They
should be the unique owners of their data, and should have the option to
opt-in or opt-out regarding sharing their data.

Finally, the log files also contain data about the teachers or the
teaching team, for instance, the average response time for forum
postings. Here also, this data constitutes a potential source of
analytics that might contribute to quality management. Globally, MOOCs
produce an unprecedented degree of transparency in teaching. However,
from a trade union perspective this may not be accepted (potential of
directly measuring the productivity and quality of work). A monitoring
of educational resources under the defined responsibility of teachers
might be more acceptable: If there is a problem with a resource, the
responsible teacher may be alerted and asked to ``take care of issues''
(e.g. lack of responses to student questions).


\subsection{Sharing MOOC data across institutions}

Currently, a university receives from the MOOC platform the data from
the MOOCs that have been used by that university. However, the research
community would be very interested in the possibility of making data
from MOOCs in some form available to all. Many scientific domains have
been boosted since large data sets have been made available
worldwide. One solution would be that the terms of services include the
possibility to provide data to non-profit educational institutions, only
for research purposes. Such an agreement would need to be added to the
contracts that universities have with MOOC providers. There is a
potential of abuse, though - consider that most whales today are killed
officially for research purposes but most universities have an internal
review board who could check the TODO

Sharing data requires of course a full anonymisation of this data which
is a hard problem in computer science in itself and difficult to
implement in practice, see for instance the Netflix and AOL search data
leaks. While it is easy to remove names from log files, the forum
postings may include information that enables third parties to directly
(e.g. through signatures, nicknames and other keys) or indirectly
(e.g. a chemist woman from Lausanne) identify the learner. Arguably, it
is the learner's responsibility not to put in a public forum the
information that he considers as confidential. However, for the users it
is often unforeseeable what type of information may lead to which
privacy leak.

In certain cases, it is mandatory that individual participants can be
identified. For instance, in a teacher training MOOC, a pre-service
teacher would hardly be able to describe an example of classroom
conflict without providing any confidential information. From an
analytics point of view it is important that anonymisation maintains all
data from the same learner associated to the same ID. This, in turn,
would imply that user identity resolved from a forum posting would also
allow for de-anonymising test results later on. We recommend to support
research projects to invent optimal solutions to this trade-off between
the interest data sharing and privacy concerns.

One condition for sharing data across MOOCs and across platforms is that
the context-specific meaning of collected should not be lost during that
transfer. An example is that MOOCs contain many different types of
quizzes: some attention-enhancer quizzes simply check the understanding
of the last video segment and can be answered in a few seconds, while
other quizzes may propose several solutions to a complex problem that
may require several hours of work. It would not make sense to compare
the success rate of such two quizzes or to compute an average response
rate across them. Sharing MOOC data will only be useful if it is
accompanied by a semantic description of the data collected, i.e. a
description of what did this data mean in the original context. Such a
description would also be necessary for the ethical issues mentioned
above. We recommend to support research efforts in that direction.


\subsection{Conclusions}

Currently, universities negotiate one-by-one with MOOC platforms, which
puts them in a rather weak position, despite the fact that they provide
content that have been developed with their own (public) funding over
many years.  For the two data issues we discussed in this section, we
recommend to move towards a collective negotiation, at national level
for countries that have specific privacy laws, may be at European level
for countries that have rather similar laws. To reach this goal, we
recommend that associations of Universities collect data privacy and
data ownership concerns across their members institutions.

